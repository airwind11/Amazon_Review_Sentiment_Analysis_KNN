{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from collections import Counter\n",
    "from stemming.porter2 import stem\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def PreProcess_train_File(file):\n",
    "    with open(file,'r') as f:\n",
    "        df = pd.DataFrame(l.split(\"\\t\") for l in f) \n",
    "        newcols = {0: 'SentimentClass',1: 'Review',}\n",
    "        df.rename(columns=newcols, inplace=True)\n",
    "\n",
    "    clean = re.compile('<.*?>')\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x:re.sub(clean, ' ', x))\n",
    "    df[\"SentimentClass\"] = df[\"SentimentClass\"].apply(lambda x:1 if x==\"+1\" else -1)\n",
    "    df[\"Review\"] = df[\"Review\"].str.lower().str.split()\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x: [item for item in x if item not in stop])\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x: [re.sub(\"[^a-z]+\", \"\", word) for word in x if re.search(\"[^0-9]\",word)<>None])\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x:[stem(t) for t in x ])\n",
    "    print df.get_value(0,'Review')\n",
    "    print df.get_value(0,'SentimentClass')\n",
    "    return df\n",
    "\n",
    "def PreProcess_test_File(file):\n",
    "    with open(file,'r') as f:\n",
    "        df = pd.DataFrame(l for l in f) \n",
    "        newcols = {0: 'Review',}\n",
    "        df.rename(columns=newcols, inplace=True)\n",
    "\n",
    "    clean = re.compile('<.*?>')\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x:re.sub(clean, ' ', x))\n",
    "    df[\"Review\"] = df[\"Review\"].str.lower().str.split()\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x: [item for item in x if item not in stop])\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x: [re.sub(\"[^a-z]+\", \"\", word) for word in x if re.search(\"[^0-9]\",word)<>None])\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x:[stem(t) for t in x ])\n",
    "    print df.get_value(0,'Review')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "    \n",
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "    \n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale matrix and normalize its rows\n",
    "from collections import defaultdict\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            round(val[j], 4)\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = PreProcess_train_File('train.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = PreProcess_test_File('test.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = build_matrix(train_df[\"Review\"])\n",
    "test_mat = build_matrix(test_df[\"Review\"])\n",
    "print train_mat.get_shape()\n",
    "print test_mat.get_shape()\n",
    "#reshape(shape[, order])\n",
    "train_mat_csr_idf = csr_idf(train_mat, copy=True)\n",
    "test_mat_csr_idf = csr_idf(test_mat, copy=True)\n",
    "train_mat_csr_idf_norm = csr_l2normalize(train_mat_csr_idf, copy=True)\n",
    "test_mat_csr_idf_norm = csr_l2normalize(test_mat_csr_idf, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dp2 = np.dot(test_mat_csr_idf_norm,train_mat_csr_idf_norm.T)\n",
    "#print (test_mat_csr_idf_norm.get_shape())\n",
    "#print (train_mat_csr_idf_norm.get_shape())\n",
    "#print (test_mat_csr_idf_norm[0].toarray())\n",
    "#print (test_mat_csr_idf_norm[0:2].toarray())\n",
    "classification_list = []\n",
    "\n",
    "for doc in test_mat_csr_idf_norm:\n",
    "    Similarity_Scores = {}\n",
    "    list_of_Scores = []\n",
    "    \n",
    "    for doc1 in train_mat_csr_idf_norm:\n",
    "        list_of_Scores.append(doc.dot(doc1.T).todense().item())\n",
    "    Similarity_Scores[0] = list(enumerate(list_of_Scores))\n",
    "\n",
    "    classification=\"\"\n",
    "    knnscore=0\n",
    "    sortedSimilarityScores = sorted(Similarity_Scores[0], key=lambda x: x[1],reverse=True)\n",
    "\n",
    "    for df_index,cosineSimilarity in sortedSimilarityScores[0:20]:\n",
    "        weighted_classification = (train_df.loc[df_index, 'SentimentClass'])/(cosineSimilarity*cosineSimilarity)    \n",
    "        knnscore += weighted_classification\n",
    "    classification_list.append(knnscore)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"end of computation\")    \n",
    "with open('format.dat', 'w') as f:\n",
    "        for item in classification_list:\n",
    "            if item>0:\n",
    "                f.write('+1\\n')\n",
    "            else:\n",
    "                 f.write('-1\\n')\n",
    "            f.close\n",
    "print (\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
