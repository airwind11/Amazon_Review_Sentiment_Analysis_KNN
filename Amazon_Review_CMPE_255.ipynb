{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from collections import Counter\n",
    "from stemming.porter2 import stem\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def PreProcess_train_File(file):\n",
    "    with open(file,'r') as f:\n",
    "        df = pd.DataFrame(l.split(\"\\t\") for l in f) \n",
    "        newcols = {0: 'SentimentClass',1: 'Review',}\n",
    "        df.rename(columns=newcols, inplace=True)\n",
    "\n",
    "    clean = re.compile('<.*?>')\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x:re.sub(clean, ' ', x))\n",
    "    df[\"SentimentClass\"] = df[\"SentimentClass\"].apply(lambda x:1 if x==\"+1\" else -1)\n",
    "    df[\"Review\"] = df[\"Review\"].str.lower().str.split()\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x: [item for item in x if item not in stop])\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x: [re.sub(\"[^a-z]+\", \"\", word) for word in x if re.search(\"[^0-9]\",word)<>None])\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x:[stem(t) for t in x ])\n",
    "    print df.get_value(0,'Review')\n",
    "    print df.get_value(0,'SentimentClass')\n",
    "    return df\n",
    "\n",
    "def PreProcess_test_File(file):\n",
    "    with open(file,'r') as f:\n",
    "        df = pd.DataFrame(l for l in f) \n",
    "        newcols = {0: 'Review',}\n",
    "        df.rename(columns=newcols, inplace=True)\n",
    "\n",
    "    clean = re.compile('<.*?>')\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x:re.sub(clean, ' ', x))\n",
    "    df[\"Review\"] = df[\"Review\"].str.lower().str.split()\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x: [item for item in x if item not in stop])\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x: [re.sub(\"[^a-z]+\", \"\", word) for word in x if re.search(\"[^0-9]\",word)<>None])\n",
    "    df[\"Review\"] = df[\"Review\"].apply(lambda x:[stem(t) for t in x ])\n",
    "    print df.get_value(0,'Review')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "    \n",
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "    print idx\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scale matrix and normalize its rows\n",
    "from collections import defaultdict\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "           \n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravi\\AppData\\Roaming\\Python\\Python27\\site-packages\\ipykernel_launcher.py:23: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['although', 'film', 'bruce', 'willi', 'alway', 'worth', 'watch', 'better', 'skip', 'one', 'watch', 'one', 'televis', 'didnt', 'plunk', 'cash', 'it', 'lucki', 'me', 'plot', 'develop', 'slowli', 'slowli', 'although', 'first', 'minut', 'quit', 'believ', 'get', 'unbeliev', 'toward', 'end', 'high', 'question', 'season', 'soldier', 'like', 'lt', 'water', 'would', 'disobey', 'direct', 'order', 'even', 'would', 'rest', 'platoon', 'would', 'know', 'put', 'direct', 'danger', 'know', 'certain', 'die', 'follow', 'him', 'heck', 'lt', 'let', 'say', 'despit', 'direct', 'order', 'rememb', 'still', 'nice', 'scene', 'movi', 'somewhat', 'save', 'villag', 'total', 'popul', 'massacr', 'rebel', 'well', 'save', 'dozen', 'villag', 'so', 'rest', 'alreadi', 'kill', 'strang', 'part', 'it', 'take', 'truck', 'rebel', 'left', 'behind', 'rather', 'go', 'foot', 'mayb', 'road', 'unsaf', 'explan', 'it', 'anyway', 'think', 'earn', 'movi', 'one', 'point', 'gave', 'it', 'made', 'movi', 'insult', 'brain', 'henc', 'complet', 'unbeliev', 'group', 'soldier', 'kill', 'mani', 'rebel', 'without', 'hurt', 'kill', 'themselv', 'near', 'end', 'loos', 'comrad', 'fight', 'armi', 'near', 'more', 'believ', 'that', 'fight', 'armi', 'mani', 'kill', 'hundr', 'them', 'loos', 'themselv', 'round', 'round', 'ammo', 'never', 'run', 'it', 'grenad', 'claymor', 'mine', 'm', 'machin', 'gun', 'even', 'rpg', 'get', 'stuff', 'carri', 'around', 'what', 'even', 'got', 'laptop', 'show', 'activ', 'enemi', 'rebel', 'laptop', 'batteri', 'goe', 'day', 'realli', 'think', 'crap', 'guess', 'turn', 'brain', 'complet', 'accept', 'rebel', 'bunch', 'idiot', 'give', 'movi', 'high', 'rate', 'not', 'skip', 'one', 'save', 'time']\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "train_df = PreProcess_train_File('train.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravi\\AppData\\Roaming\\Python\\Python27\\site-packages\\ipykernel_launcher.py:41: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glad', 'watch', 'everi', 'time', 'movi', 'ray', 'sidney', 'pottier', 'st', 'centuri', 'jami', 'foxx', 'play', 'role', 'evid', 'brilliant', 'abil', 'actor', 'take', 'posit', 'besid', 'great', 'actor', 'hollywood', 'golden', 'support', 'strong', 'abil', 'afro', 'american', 'actor', 'time', 'period', 'etern', 'work', 'evid', 'etern', 'ray', 'charl', 'grand', 'prove', 'legend', 'appear', 'everi', 'time', 'success', 'win', 'oscar', 'prize', 'best', 'actor', 'lead', 'role', 'best', 'mix', 'sound', 'fact', 'cinemat', 'scene', 'upon', 'ray', 'jami', 'mix', 'two', 'copi', 'ray', 'ray', 'past', 'ray', 'act', 'person', 'jami', 'foxxit', 'nice', 'director', 'choos', 'song', 'adapt', 'dramat', 'scene', 'accid', 'film', 'enter', 'atmospher', 'success', 'legend', 'corn', 'legend', 'movi', 'hollywood', 'sinc', 'till', 'clever', 'choos', 'sharon', 'warrn', 'role', 'ray', 's', 'mother', 'succeed', 'role', 'brilliant', 'analysi', 'core', 'charact', 'ray', 'mother', 'turn', 'point', 'upon', 'grew', 'independ', 'take', 'place', 'world', 'icon', 'talent', 'nat', 'king', 'cole', '', 'loui', 'armstrong', '', 'duke', 'ellington', 'end', 'film', 'receiv', 'honor', 'report', 'gerogia', 'decis', 'take', 'song', 'georgia', 'mind', 'nation', 'anthem', 'state', 'made', 'promis', 'mother', 'aliv', 'legend', 'song', 'brilliant', 'life']\n"
     ]
    }
   ],
   "source": [
    "test_df = PreProcess_test_File('test.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 123451)\n"
     ]
    }
   ],
   "source": [
    "trainlist = train_df[\"Review\"].tolist()\n",
    "testlist = test_df[\"Review\"].tolist()\n",
    "totallist = trainlist + testlist\n",
    "joint_mat = build_matrix(totallist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "round not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-39646bfae865>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0midf_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoint_mat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0midf_normal_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_l2normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midf_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0midf_normal_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0midf_normal_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: round not found"
     ]
    }
   ],
   "source": [
    "idf_matrix = csr_idf(joint_mat, copy=True)\n",
    "idf_normal_matrix = csr_l2normalize(idf_matrix, copy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07878077  0.00842491  0.07411369 ...,  0.          0.          0.        ]]\n",
      "[[ 0.079  0.008  0.074 ...,  0.     0.     0.   ]]\n"
     ]
    }
   ],
   "source": [
    "rounded_array = np.around(idf_normal_matrix,3)\n",
    "#print idf_normal_matrix[0].todense()\n",
    "#print rounded_array[0].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dp2 = np.dot(test_mat_csr_idf_norm,train_mat_csr_idf_norm.T)\n",
    "#print (test_mat_csr_idf_norm.get_shape())\n",
    "#print (train_mat_csr_idf_norm.get_shape())\n",
    "#print (test_mat_csr_idf_norm[0].toarray())\n",
    "#print (test_mat_csr_idf_norm[0:2].toarray())\n",
    "classification_list = []\n",
    "\n",
    "for doc in rounded_array[25000:49999]:\n",
    "    Similarity_Scores = {}\n",
    "    list_of_Scores = []\n",
    "    \n",
    "    for doc1 in rounded_array[0:24999]:\n",
    "        list_of_Scores.append(doc.dot(doc1.T).todense().item())\n",
    "    Similarity_Scores[0] = list(enumerate(list_of_Scores))\n",
    "\n",
    "    classification=\"\"\n",
    "    knnscore=0\n",
    "    sortedSimilarityScores = sorted(Similarity_Scores[0], key=lambda x: x[1],reverse=True)\n",
    "\n",
    "    for df_index,cosineSimilarity in sortedSimilarityScores[0:20]:\n",
    "        if cosineSimilarity==0:\n",
    "            break\n",
    "        else:\n",
    "            weighted_classification = (train_df.loc[df_index, 'SentimentClass'])/(cosineSimilarity*cosineSimilarity)    \n",
    "            knnscore += weighted_classification\n",
    "    classification_list.append(knnscore)\n",
    "print (\"end of computation\")    \n",
    "with open('format.dat', 'w') as f:\n",
    "        for item in classification_list:\n",
    "            if item>0:\n",
    "                f.write('+1\\n')\n",
    "            else:\n",
    "                 f.write('-1\\n')\n",
    "            f.close\n",
    "print (\"end\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4763"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classification_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
